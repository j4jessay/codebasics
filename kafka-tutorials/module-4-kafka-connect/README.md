# Module 4: Kafka Connect & Azure Data Factory

## â±ï¸ Duration: 90 minutes
**Theory: 40 min | Hands-On: 50 min**

---

## ğŸ¯ Learning Objectives

By the end of this module, you will be able to:

### Part A: Kafka Connect (30 min)
- âœ… Understand Kafka Connect architecture and benefits
- âœ… Explain the difference between source and sink connectors
- âœ… Configure Azure Blob Storage sink connector
- âœ… Deploy connectors using REST API
- âœ… Export streaming data to cloud storage with Parquet format
- âœ… Verify data in Azure with time-based partitioning

### Part B: Azure Data Factory (60 min)
- âœ… Understand Azure Data Factory architecture and use cases
- âœ… Design ETL pipelines for data warehouse loading
- âœ… Configure ADF linked services and datasets
- âœ… Build copy pipelines to load data from Blob to Synapse
- âœ… Implement data validation and staging patterns
- âœ… Set up event-based and scheduled triggers
- âœ… Monitor pipeline execution and troubleshoot issues

---

## ğŸ“š Module Structure

### Part A: Kafka Connect (30 minutes)

#### Theory (15 minutes)

Read the following theory files in order:

1. **[What is Kafka Connect?](theory/01-what-is-kafka-connect.md)** (5 min)
   - Connect architecture
   - Why use connectors vs writing code
   - Connect cluster and workers

2. **[Source vs Sink Connectors](theory/02-source-vs-sink.md)** (5 min)
   - Source connectors (data import)
   - Sink connectors (data export)
   - Popular connectors

3. **[Connector Configuration](theory/03-connector-configuration.md)** (5 min)
   - Configuration parameters
   - REST API for deployment
   - Monitoring and troubleshooting

#### Hands-On Lab (15 minutes)

**[â†’ Go to Kafka Connect Lab](lab/README.md#kafka-connect-exercises)**

- Set up Azure Storage Account (optional - can use ARM template)
- Configure Azure Blob Storage connector (10 min)
- Deploy connector and verify Parquet files (5 min)

---

### Part B: Azure Data Factory (60 minutes)

#### Theory (25 minutes)

4. **[Azure Data Factory Overview](theory/04-azure-data-factory.md)** (25 min)
   - ADF architecture and components
   - Linked services, datasets, pipelines, activities
   - Triggers (schedule, tumbling window, event-based)
   - Integration with Kafka pipeline
   - Data transformation options
   - Monitoring and debugging
   - Best practices and cost optimization

#### Hands-On Lab (35 minutes)

**[â†’ Go to ADF Lab](lab/adf-pipelines/README.md)**

1. **Deploy Azure Resources** (10 min)
   - Run ARM template deployment script
   - Create resource group with Storage, ADF, and Synapse

2. **Configure ADF** (10 min)
   - Create linked services (Blob + Synapse)
   - Create datasets (Parquet source + SQL sink)
   - Import pipeline templates

3. **Test Pipelines** (10 min)
   - Debug simple copy pipeline
   - Test staged load with validation
   - Monitor pipeline execution

4. **Configure Triggers** (5 min)
   - Set up event-based trigger for blob creation
   - Create scheduled trigger for daily batch loads

---

## âœ… Prerequisites

Before starting this module:

- [ ] Completed Module 1, 2, and 3
- [ ] Have producer and ksqlDB streams running from previous modules
- [ ] **Azure account** (free trial or subscription)
- [ ] **Azure CLI** installed ([Installation guide](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli))
- [ ] Understand JSON configuration files
- [ ] Basic SQL knowledge (for Synapse in Module 6)
- [ ] Estimated Azure cost: **$45-100/month** (can pause resources to save costs)

---

## ğŸš€ What You'll Build

In this module, you'll build a complete ETL pipeline from Kafka to Azure Synapse Analytics:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PIPELINE ARCHITECTURE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Part A: Kafka Connect (Data Export)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   ksqlDB Streams   â”‚
  â”‚                    â”‚
  â”‚ â€¢ vehicle.speeding â”‚
  â”‚ â€¢ vehicle.lowfuel  â”‚
  â”‚ â€¢ vehicle.stats    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚Kafka Connect â”‚
     â”‚              â”‚
     â”‚ Azure Blob   â”‚
     â”‚    Sink      â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Export Parquet (every 5 min)
            â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Azure Blob Storage    â”‚
  â”‚  (Bronze Layer)       â”‚
  â”‚                       â”‚
  â”‚ /vehicle-telemetry/   â”‚
  â”‚   2025/11/17/10/      â”‚
  â”‚     *.parquet         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚
Part B: Azure Data Factory (ETL Orchestration)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

          â”‚ Trigger (event/schedule)
          â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Azure Data Factory â”‚
  â”‚  ETL Pipeline      â”‚
  â”‚                    â”‚
  â”‚ â€¢ Copy Activity    â”‚
  â”‚ â€¢ Validation       â”‚
  â”‚ â€¢ Transform        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Load data
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Azure Synapse      â”‚
  â”‚  Analytics         â”‚
  â”‚  (Gold Layer)      â”‚
  â”‚                    â”‚
  â”‚ Star Schema:       â”‚
  â”‚ â€¢ Fact tables      â”‚
  â”‚ â€¢ Dimensions       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚    Power BI        â”‚
  â”‚   Dashboards       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**By the end, you'll have**:
- âœ… Automated data export from Kafka to Azure Blob (Parquet format)
- âœ… Scheduled ETL pipelines to load data into Synapse
- âœ… Event-triggered pipelines (run when new files arrive)
- âœ… Data validation and quality checks
- âœ… Foundation for analytics and reporting (Module 6)

---

## ğŸ“ Success Criteria

You've successfully completed this module when you can:

### Part A: Kafka Connect
- [ ] Explain what Kafka Connect is and its benefits
- [ ] Differentiate between source and sink connectors
- [ ] Configure the Azure Blob Storage sink connector
- [ ] Deploy connector using the REST API
- [ ] Verify Parquet files in Azure Blob Storage
- [ ] Understand time-based partitioning (year/month/day/hour)

### Part B: Azure Data Factory
- [ ] Explain ADF architecture (linked services, datasets, pipelines, activities)
- [ ] Deploy Azure resources using ARM templates
- [ ] Create linked services for Blob Storage and Synapse
- [ ] Import and configure ADF pipeline templates
- [ ] Run a successful debug execution of a copy pipeline
- [ ] Understand data validation patterns (staging tables)
- [ ] Configure event-based or scheduled triggers
- [ ] Monitor pipeline runs in ADF UI
- [ ] Troubleshoot common ADF errors (permissions, schema mismatches)

---

## â­ï¸ Next Module

Once you've completed this module, proceed to:

**[Module 5: Monitoring & Operations â†’](../module-5-monitoring-operations/)**

(Or skip ahead to **[Module 6: Complete Pipeline with Synapse â†’](../module-6-complete-pipeline/)** to continue building the Azure data warehouse)

---

## ğŸ†˜ Need Help?

### Kafka Connect
- **[Kafka Connect Documentation](https://docs.confluent.io/platform/current/connect/)**
- **[Azure Blob Sink Connector](https://docs.confluent.io/kafka-connectors/azure-blob-storage-sink/current/overview.html)**

### Azure Data Factory
- **[ADF Documentation](https://learn.microsoft.com/en-us/azure/data-factory/)**
- **[ADF Pipeline Troubleshooting](lab/adf-pipelines/README.md#troubleshooting)**
- **[ARM Template Deployment Guide](lab/arm-templates/README.md)**

### Common Issues
- **Kafka Connect not starting**: Check Docker logs and REST API connectivity
- **ADF permission errors**: Verify managed identity RBAC roles
- **Synapse connection fails**: Check firewall rules and SQL admin password
- **High Azure costs**: Pause Synapse SQL pool when not in use

---

## ğŸ’¡ Tips for Success

1. **Complete Part A first** - Get Kafka Connect working before tackling ADF
2. **Use ARM template** - Automated deployment saves time and reduces errors
3. **Pause resources** - Pause Synapse SQL pool between lab sessions to save ~$200/month
4. **Start small** - Begin with simple copy pipeline, then add validation
5. **Monitor costs** - Set up budget alerts in Azure Cost Management
6. **Keep notes** - Save resource names and connection strings in `deployment-config.txt`

---

**Let's begin!** Start with Part A theory files, then move to the labs.
